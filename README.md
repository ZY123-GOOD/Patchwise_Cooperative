# Patchwise-Cooperative-Game-based-Interpretability-Method-for-Large-Vision-Language-Models

This repository contains the code for the interpretability method described in the paper "Patchwise Cooperative Game-based Interpretability Method for Large Vision-Language Models." It includes example usages of the interpretability method, the implementation of the ROAD faithfulness evaluation method, and instructions for using the Human Response Interpretability Dataset. For detailed usage, please refer to the Instructions.ipynb file.

---

## Models Used in the Paper

The models used in the paper can be directly loaded and used. Their specific information is as follows. You can download them from [Hugging Face](https://huggingface.co/):

### Model Information
* **llava-v1.5-13b**
  * ​**Tokenizer Path**​: `liuhaotian/llava-v1.5-13b`
  * ​**Model Path**​: `liuhaotian/llava-v1.5-13b`
  * ​**Conversation Mode**​: Chat
  * ​**Image Aspect Ratio**​: Pad
* **llava-v1.5-7b**
  * ​**Tokenizer Path**​: `liuhaotian/llava-v1.5-7b`
  * ​**Model Path**​: `liuhaotian/llava-v1.5-7b`
  * ​**Conversation Mode**​: Chat
  * ​**Image Aspect Ratio**​: Pad
* **sharegpt4v-7b**
  * ​**Tokenizer Path**​: `Lin-Chen/ShareGPT4V-7B`
  * ​**Model Path**​: `Lin-Chen/ShareGPT4V-7B`
  * ​**Conversation Mode**​: Chat
  * ​**Image Aspect Ratio**​: Pad
* **sharegpt4v-13b**
  * ​**Tokenizer Path**​: `Lin-Chen/ShareGPT4V-13B`
  * ​**Model Path**​: `Lin-Chen/ShareGPT4V-13B`
  * ​**Conversation Mode**​: Chat
  * ​**Image Aspect Ratio**​: Pad

---

## Other Models

For other models not listed here, you may need to adjust the configurations according to your specific requirements.

---

## Getting Started

For detailed instructions on how to use the interpretability method, evaluate faithfulness using ROAD, and work with the Human Response Interpretability Dataset, please refer to the drawattn_demo.ipynb file.

---

## Contact

If you have any questions or need further assistance, feel free to contact us via email ee_zhuy@zju.edu.cn or create an issue in this repository.

Thank you for your interest in our work!
